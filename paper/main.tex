\documentclass{article}

\usepackage{fullpage}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorlinks,linkcolor=dblue,filecolor=black,citecolor=dblue,urlcolor=dblue]{hyperref}
\usepackage{etex}
\bibliographystyle{alpha}
\usepackage[english]{babel}
\usepackage{amsfonts,amssymb,amsmath,sectsty,url}
\usepackage{mathrsfs}

\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{color}
%\usepackage{mathtools}
\usepackage[all]{xy}
\usepackage{ctable}
% \usetikzlibrary{calc}

\usepackage{graphicx}
\usepackage{ifpdf}
\usepackage{multirow}
\usepackage{multicol,color}
\usepackage{tablefootnote}


\usepackage{amsthm}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}%[section]
\newtheorem{proposition}[theorem]{Proposition}%[section]

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}%[section]
\newtheorem{construction}[theorem]{Construction}


\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{fact}[theorem]{Fact}%[section]
\newtheorem{observation}[theorem]{Observation}
\newtheorem{claim}[theorem]{Claim}%[section]



\input{symbols}


\newcommand{\mr}[1]{{\textcolor{lblue}{\textbf{Mariana:} #1}}}
\newcommand{\vc}[1]{{\textcolor{lred}{\textbf{Valerie:} #1}}}
\newcommand{\vp}[1]{{\textcolor{dblue}{\textbf{Valerio:} #1}}}

\title{Multiparty Linear Regression via SPDZ}
\author{Valerie Chen \and Valerio Pastro \and Mariana Raykova}


\begin{document}

\titlepage
\maketitle

\pagebreak

\section{Preliminaries}
\vp{stuff}

\section{Experiments}
The experiments were run on the SPDZ framework, using a 128 bit and 256 bit prime field respectively. In the 128 bit prime field, the bits were allotted with 28 bits after the decimal place to match the 32 bit example in the original paper and 60 bits after the decimal for the original 64 bit case. Compared original paper, we needed more space to the left of the decimal for comparison purposes and operations using the fixed point type. However, we believe this is still a fair comparison because we are not working with more precision after the decimal place and if anything using more bits should actually slow down our calculations. For each generated matrix of varying dimensions, the features were split evenly between two parties. We then wanted to see if linear decomposition algorithms like LDLT, Cholesky, and CGD (with 5, 10, 15, and 20 iterations) were able to get comparable if not better results. 

First, we ran the two algorithms two gauge time and error rates for matrices of size 10, 20, 50, 100, 200, and 500, with 200000 features each. Secondly, we ran on a size 20 matrices with condition numbers varying from 1 to 10. Both parts were completed for both bit sizes to evaluate performance differences. To model after the original experiment, we ran iterations of 5, 10, 15, and 20 for the CGD algorithm but added on as an extension of 25 iterations to see if necessary (i.e. the errors were not comparable for all sizes or condition numbers), there was still room for improvement. 

\section{Result}

\begin{figure}[h]
  \includegraphics[width=\linewidth]{results.jpg}
  \caption{(Left) Comparison between different methods of solving linear systems: running time in seconds for Cholesky, LDLT, and CGD (with 5, 10, 15, and 20 iterations) as a function of input dimension. (Middle) Accuracy of CGD as a function of the input on the condition number of the input matrix A with d = 20. (Right) Accuracy of Cholesky, LDLT, and CGD, as a function of the input dimensionality d. (Top) Fixed-point numbers with the b = 80 bits, with 60 in the fractional part. (Bottom) b = 41, with 28 in the fractional part}
  \label{fig:result1}
\end{figure}

As expected, utilizing more bits gives much better accuracy. In both cases, the error results are comparable to the garbled circuits paper. However, we did find that the original experiment truncated the generated data at 12 decimal places, however we modified it so that we could work with the full representation of the number to get a better understanding of how accurate our algorithm is.  

The run times for SPDZ are always comparable if not much faster. In terms of run-time, we observe that both Cholesky and LDLT were faster for smaller dimensions when d $\leq$ 100 and CGD is faster for larger dimensions when d > 100. This is consistent with the findings in the original experiment. For dimensions 100 and smaller, when Cholesky and LDLT are faster, they comparative differences are not that big (LDLT with 1.6 seconds compared to CGD 20 with 60.89 seconds) when compared to the largest differences between the larger sizes (LDLT with 1747 seconds and CGD5 with 97 seconds).

[insert table comparison here??]

In the condition number experiment, for CGD iterations of 5, 10, and 15, error increases as condition number increases. For example, in CGD 10 a small condition number may have an error of 10e-9 yet a larger condition number could have an error as large as 10e-3. However, in the CGD 20 case, the condition number does not seem to have as much of effect as the range of errors is much less., with the error varying only form 10e-11 to 10e-14 keeping in mind these are much smaller increments. (why?? Add some explanation about how condition number affects errors in a matrix) For the 60 bit precision case, we chose to run more iterations because in the CGD 20 line, there is a noticeable upward slope in the scatter plot, indicating that still as condition numbers increased, the errors were increasing as well. Indeed, 25 iterations showed some improvement as we can see a flatter trend in the points. 

\end{document}









